{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2ca263-6525-42ff-b1fb-bb505a12e668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se hizo clic en el botón de aceptar cookies\n",
      "Scraping página 1...\n",
      "Scraping página 2...\n",
      "Scraping página 3...\n",
      "Scraping página 4...\n",
      "Scraping página 5...\n",
      "Scraping página 6...\n",
      "Scraping página 7...\n",
      "Scraping página 8...\n",
      "Scraping página 9...\n",
      "Scraping página 10...\n",
      "Scraping página 11...\n",
      "Scraping página 12...\n",
      "Scraping página 13...\n",
      "Scraping página 14...\n",
      "Scraping página 15...\n",
      "Scraping página 16...\n",
      "Scraping página 17...\n",
      "Scraping página 18...\n",
      "Scraping página 19...\n",
      "Scraping página 20...\n",
      "Scraping página 21...\n",
      "Scraping página 22...\n",
      "Scraping página 23...\n",
      "Scraping página 24...\n",
      "Scraping página 25...\n",
      "Scraping página 26...\n",
      "Scraping página 27...\n",
      "Scraping página 28...\n",
      "Scraping página 29...\n",
      "Scraping página 30...\n",
      "Scraping página 31...\n",
      "Scraping página 32...\n",
      "Scraping página 33...\n",
      "Scraping página 34...\n",
      "Scraping página 35...\n",
      "Scraping página 36...\n",
      "Scraping página 37...\n",
      "Scraping página 38...\n",
      "Scraping página 39...\n",
      "Scraping página 40...\n",
      "Scraping página 41...\n",
      "Scraping página 42...\n",
      "Scraping página 43...\n",
      "Scraping página 44...\n",
      "Scraping página 45...\n",
      "Scraping página 46...\n",
      "Scraping página 47...\n",
      "Scraping página 48...\n",
      "Scraping página 49...\n",
      "Scraping página 50...\n",
      "Scraping página 51...\n",
      "Scraping página 52...\n",
      "Scraping página 53...\n",
      "Scraping página 54...\n",
      "Scraping página 55...\n",
      "Scraping página 56...\n",
      "Scraping página 57...\n",
      "Scraping página 58...\n",
      "Scraping página 59...\n",
      "Scraping página 60...\n",
      "Scraping página 61...\n",
      "Scraping página 62...\n",
      "Scraping página 63...\n",
      "Scraping página 64...\n",
      "Scraping página 65...\n",
      "Scraping página 66...\n",
      "Scraping página 67...\n",
      "Scraping página 68...\n",
      "Scraping página 69...\n",
      "Scraping página 70...\n",
      "Scraping página 71...\n",
      "Scraping página 72...\n",
      "Scraping página 73...\n",
      "Scraping página 74...\n",
      "Scraping página 75...\n",
      "Scraping página 76...\n",
      "Scraping página 77...\n",
      "Scraping página 78...\n",
      "Scraping página 79...\n",
      "Scraping página 80...\n",
      "Scraping página 81...\n",
      "Scraping página 82...\n",
      "Scraping página 83...\n",
      "Scraping página 84...\n",
      "Scraping página 85...\n",
      "Scraping página 86...\n",
      "Scraping página 87...\n",
      "Scraping página 88...\n",
      "Scraping página 89...\n",
      "Scraping página 90...\n",
      "Scraping página 91...\n",
      "Scraping página 92...\n",
      "Scraping página 93...\n",
      "Scraping página 94...\n",
      "Scraping página 95...\n",
      "Scraping página 96...\n",
      "Scraping página 97...\n",
      "Scraping página 98...\n",
      "Scraping página 99...\n",
      "Scraping página 100...\n",
      "Scraping página 101...\n",
      "Scraping página 102...\n",
      "Scraping página 103...\n",
      "Scraping página 104...\n",
      "Scraping página 105...\n",
      "Scraping página 106...\n",
      "Scraping página 107...\n",
      "Scraping página 108...\n",
      "Scraping página 109...\n",
      "Scraping página 110...\n",
      "Scraping página 111...\n",
      "Scraping página 112...\n",
      "Scraping página 113...\n",
      "Scraping página 114...\n",
      "Scraping página 115...\n",
      "Scraping página 116...\n",
      "Scraping página 117...\n",
      "Scraping página 118...\n",
      "Scraping página 119...\n",
      "Scraping página 120...\n",
      "Scraping página 121...\n",
      "Scraping página 122...\n",
      "Scraping página 123...\n",
      "Scraping página 124...\n",
      "Scraping página 125...\n",
      "Scraping página 126...\n",
      "Scraping página 127...\n",
      "Scraping página 128...\n",
      "Scraping página 129...\n",
      "Scraping página 130...\n",
      "Scraping página 131...\n",
      "Scraping página 132...\n",
      "Scraping página 133...\n",
      "Scraping página 134...\n",
      "Scraping página 135...\n",
      "Scraping página 136...\n",
      "Scraping página 137...\n",
      "Scraping página 138...\n",
      "Scraping página 139...\n",
      "Scraping página 140...\n",
      "Scraping página 141...\n",
      "Scraping página 142...\n",
      "Scraping página 143...\n",
      "Scraping página 144...\n",
      "Scraping página 145...\n",
      "Scraping página 146...\n",
      "Scraping página 147...\n",
      "Scraping página 148...\n",
      "Scraping página 149...\n",
      "Scraping página 150...\n",
      "Scraping página 151...\n",
      "Scraping página 152...\n",
      "Scraping página 153...\n",
      "Scraping página 154...\n",
      "Scraping página 155...\n",
      "Scraping página 156...\n",
      "Scraping página 157...\n",
      "Scraping página 158...\n",
      "Scraping página 159...\n",
      "Scraping página 160...\n",
      "Scraping página 161...\n",
      "Scraping página 162...\n",
      "Scraping página 163...\n",
      "Scraping página 164...\n",
      "Scraping página 165...\n",
      "Scraping página 166...\n",
      "Scraping página 167...\n",
      "Scraping página 168...\n",
      "Scraping página 169...\n",
      "Scraping página 170...\n",
      "Scraping página 171...\n",
      "Scraping página 172...\n",
      "Scraping página 173...\n",
      "Scraping página 174...\n",
      "Scraping página 175...\n",
      "Scraping página 176...\n",
      "Scraping página 177...\n",
      "Scraping página 178...\n",
      "Scraping página 179...\n",
      "Scraping página 180...\n",
      "Scraping página 181...\n",
      "Scraping página 182...\n",
      "Scraping página 183...\n",
      "Scraping página 184...\n",
      "Scraping página 185...\n",
      "Scraping página 186...\n",
      "Scraping página 187...\n",
      "Scraping página 188...\n",
      "Scraping página 189...\n",
      "Scraping página 190...\n",
      "Scraping página 191...\n",
      "Scraping página 192...\n",
      "Scraping página 193...\n",
      "Scraping página 194...\n",
      "Scraping página 195...\n",
      "Scraping página 196...\n",
      "Scraping página 197...\n",
      "Scraping página 198...\n",
      "Scraping página 199...\n",
      "Scraping página 200...\n",
      "Archivo parquet guardado exitosamente con el nombre: Venta_scraping_2024-06-23_16-19-43.csv\n",
      "     Fecha de Scraping                                Título  \\\n",
      "0  2024-06-23 16:19:43            Casa en Venta, Fagua, Chía   \n",
      "1  2024-06-23 16:19:43  Casa en Venta, Santa Cecilia 5, Chía   \n",
      "2  2024-06-23 16:19:43            Casa en Venta, Fagua, Chía   \n",
      "3  2024-06-23 16:19:43      Casa en Venta, Cerritos, Pereira   \n",
      "4  2024-06-23 16:19:43      Casa en Venta, Calahorra, Cajicá   \n",
      "\n",
      "                     Descripción  \n",
      "0                    245 | 5 | 5  \n",
      "1                    123 | 3 | 2  \n",
      "2                            450  \n",
      "3  $750.000.000 | 133 m² | 3 | 3  \n",
      "4  $750.000.000 | 133 m² | 3 | 3  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def scrape_website(url, pages=20):\n",
    "    driver = None  # Inicializar driver como None\n",
    "\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.headless = True  # Cambiar a False para ver la interfaz gráfica de Chrome\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--disable-popup-blocking\")\n",
    "        \n",
    "        service = Service(r'C:\\Users\\cabg1\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe')\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        driver.get(url)\n",
    "        \n",
    "        # Manejar la ventana emergente de cookies si es necesario\n",
    "        try:\n",
    "            accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, 'a.sc-bdVaJa.ebNrSm.sc-htoDjs.brhAsq.Button-bepvgg-0.dqiWxy.text-center.btn-disclaimer.btn.btn-secondary'))\n",
    "            )\n",
    "            accept_cookies_button.click()\n",
    "            print('Se hizo clic en el botón de aceptar cookies')\n",
    "        except Exception as e:\n",
    "            print(f'No se pudo aceptar cookies: {str(e)}')\n",
    "        \n",
    "        titles, descriptions = [], []\n",
    "\n",
    "        for page in range(1, pages + 1):\n",
    "            print(f\"Scraping página {page}...\")\n",
    "\n",
    "            # Esperar a que los elementos de precio y título estén presentes\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'sc-fMiknA')))\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'sc-dxgOiQ')))\n",
    "            \n",
    "            # Obtener el contenido HTML después de la carga completa\n",
    "            page_source = driver.page_source\n",
    "            \n",
    "            # Parsear el contenido HTML con BeautifulSoup\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            \n",
    "            # Encontrar todos los elementos de título\n",
    "            title_elements = soup.find_all('h2', class_='sc-dxgOiQ BSoGx card-title')\n",
    "            \n",
    "            # Encontrar todos los elementos de descripción\n",
    "            description_elements = soup.find_all('ul', class_='sc-iRbamj kYQRXi inline-list-grid')\n",
    "            \n",
    "            # Extraer y organizar los datos\n",
    "            for title_element in title_elements:\n",
    "                titles.append(title_element.div.text.strip())\n",
    "\n",
    "            for description_element in description_elements:\n",
    "                description_text = \" | \".join([item.text.strip() for item in description_element.find_all('p', class_='sc-fMiknA ZUMHA card-subitem text-black')])\n",
    "                descriptions.append(description_text)\n",
    "            \n",
    "            # Hacer clic en el botón de siguiente página si no es la última página\n",
    "            if page < pages:\n",
    "                try:\n",
    "                    next_page_number = str(page + 1)\n",
    "                    next_page_button = WebDriverWait(driver, 10).until(\n",
    "                        EC.element_to_be_clickable((By.LINK_TEXT, next_page_number))\n",
    "                    )\n",
    "                    next_page_button.click()\n",
    "                    time.sleep(3)  # Esperar unos segundos para que la página cargue\n",
    "                except Exception as e:\n",
    "                    print(f'No se pudo pasar a la siguiente página: {str(e)}')\n",
    "                    break\n",
    "\n",
    "        # Asegurarse de que todas las listas tengan la misma longitud\n",
    "        min_length = min(len(titles), len(descriptions))\n",
    "        titles = titles[:min_length]\n",
    "        descriptions = descriptions[:min_length]\n",
    "\n",
    "        # Obtener la fecha actual\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        dates = [current_date] * min_length\n",
    "        \n",
    "        # Crear un DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Fecha de Scraping': dates,\n",
    "            'Título': titles,\n",
    "            'Descripción': descriptions\n",
    "        })\n",
    "\n",
    "        # Obtener la fecha actual y formatearla\n",
    "        fecha_actual = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "        # Ruta relativa dentro de tu repositorio local clonado\n",
    "        ruta_archivo = f'C:/Users/cabg1/ScrapingMetroCuadrado/scraping/Arriendo_scraping_{fecha_actual}.csv'\n",
    "\n",
    "        # Guardar el DataFrame como archivo CSV en el repositorio local\n",
    "        df.to_csv(ruta_archivo, index=False)\n",
    "        print(f'Archivo CSV guardado exitosamente en el repositorio: {ruta_archivo}')\n",
    "\n",
    "        # Mostrar df.head() en otra línea\n",
    "        print(df.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error general: {str(e)}')\n",
    "\n",
    "    finally:\n",
    "        if driver is not None:  # Verificar si driver está definido\n",
    "            driver.quit()\n",
    "\n",
    "# Ejemplo de uso: scraping inicial de las 10 primeras páginas\n",
    "scrape_website('https://www.metrocuadrado.com/venta/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2cf31-5324-44ca-86a1-3e6489c99d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directorio donde se guardan los archivos CSV históricos (ajusta la ruta según tu estructura)\n",
    "directorio_historico = 'C:/Users/cabg1/ScrapingMetroCuadrado/scraping/'\n",
    "\n",
    "# Lista para almacenar DataFrames de todos los archivos históricos\n",
    "dfs_historicos = []\n",
    "\n",
    "# Leer archivos CSV históricos y almacenar en una lista de DataFrames\n",
    "for archivo in os.listdir(directorio_historico):\n",
    "    if archivo.endswith('.csv') and archivo.startswith('Venta_scraping'):\n",
    "        ruta_archivo = os.path.join(directorio_historico, archivo)\n",
    "        df = pd.read_csv(ruta_archivo)\n",
    "        dfs_historicos.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_historico_completoV = pd.concat(dfs_historicos, ignore_index=True)\n",
    "\n",
    "# Mostrar información básica del DataFrame histórico completo\n",
    "print(\"Información del DataFrame histórico completo:\")\n",
    "print(df_historico_completoV.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5bd523-ba24-473e-b61e-7f6cc679e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una clave única basada en las características clave del inmueble\n",
    "df_historico_completoV['Clave_Unica'] = (\n",
    "    df_historico_completoV['Fecha de Scraping'].astype(str) + '_' +\n",
    "    df_historico_completoV['Título'].astype(str) + '_' +\n",
    "    df_historico_completoV['Descripción'].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38aa2b0-682b-45f1-91ab-6be9047c81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_historico_completo[['Clave_Unica', 'Fecha de Scraping', 'Título', 'Descripción']].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a32ca-2c1a-4adc-bdb4-f9525e402bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filas = df.shape[0]\n",
    "print(f'El DataFrame tiene {num_filas} filas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b0d9d-14a0-4ef5-93e5-f39151ddb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fecha de Scraping'] = pd.to_datetime(df['Fecha de Scraping'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce33d0-1d8c-4af6-b851-9420cd580b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "registros_iniciales = len(df)\n",
    "\n",
    "df = df.drop_duplicates(subset='Clave_Unica', keep='first')\n",
    "\n",
    "registros_eliminados = registros_iniciales - len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de81b4-8804-4c51-926f-0e2768533212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Se eliminaron {registros_eliminados} registros duplicados de la fecha de scraping más reciente.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb16010-8a4c-4987-aee7-41542a902d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con NaN en la columna 'descripcion'\n",
    "df = df.dropna(subset=['Descripción'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ad890-f4ff-4839-91b6-6179eae944e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar la columna 'Título' en nuevas columnas: Tipo de Inmueble, Sector, Ciudad\n",
    "df[['Tipo de Inmueble', 'Sector', 'Ciudad']] = df['Título'].str.split(',', expand=True)\n",
    "\n",
    "# Separar la columna 'Descripción' en nuevas columnas: Precio, Superficie, Habitaciones, Baños\n",
    "df[['Precio', 'Superficie', 'Habitaciones', 'Baños']] = df['Descripción'].str.split('|', expand=True)\n",
    "\n",
    "# Limpieza adicional para eliminar espacios en blanco alrededor de cada valor\n",
    "df['Tipo de Inmueble'] = df['Tipo de Inmueble'].str.strip()\n",
    "df['Sector'] = df['Sector'].str.strip()\n",
    "df['Ciudad'] = df['Ciudad'].str.strip()\n",
    "df['Precio'] = df['Precio'].str.strip()\n",
    "df['Superficie'] = df['Superficie'].str.strip()\n",
    "df['Habitaciones'] = df['Habitaciones'].str.strip()\n",
    "df['Baños'] = df['Baños'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386124ba-10bb-4f96-91a8-56eddf804048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear df1 eliminando las filas donde 'Baños' contiene NaN o None\n",
    "df = df.dropna(subset=['Baños'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bceb1a-80a3-4973-a22c-235d42d3071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear df1 eliminando las columnas 'Título' y 'Descripción'\n",
    "df1 = df.drop(['Título', 'Descripción'], axis=1)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa6462-4573-4d26-82fb-0394cb25aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar registros por ciudad y obtener el top 10\n",
    "top_20_ciudades = df1['Ciudad'].value_counts().head(20)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(top_20_ciudades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8b8652-731d-4193-929c-cae3f8063cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ciudades especificadas\n",
    "ciudades_especificadas = ['Bogotá D.C.', 'Medellín', 'Envigado', 'Cali', 'Barranquilla', 'Sabaneta',\n",
    "                          'Manizales', 'Pereira', 'Chía', 'Rionegro', 'Cartagena de Indias']\n",
    "\n",
    "# Filtrar el DataFrame original para incluir solo las ciudades especificadas\n",
    "df_filter = df1[df1['Ciudad'].isin(ciudades_especificadas)]\n",
    "\n",
    "# Mostrar el DataFrame filtrado\n",
    "print(df_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf0160-3248-4c6b-b99a-5dadf58bbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
